# Qwen-0.5B-Chat LoRA 微调探索：一个关于个人知识注入的深度实践案例

## 项目概述

本项目旨在通过 **LoRA (Low-Rank Adaptation)** 对 **Qwen-0.5B-Chat** 模型进行参数高效微调，目标是向模型注入关于一个特定人物“hiahia”的个人知识库，使其能够作为一个专属AI助手，准确回答相关问题。

整个项目从零开始，实现了一个完整的LoRA微调流水线，包括：
- **动态LoRA层注入**：无需修改模型源码，将LoRA模块动态注入到Transformer模型的任意线性层。
- **数据处理与模板化**：支持自定义问答数据集，并兼容Qwen的ChatML聊天模板。
- **训练与评估**：包含完整的训练、验证循环，并能根据验证集表现自动保存最佳模型。
- **损失可视化**：自动绘制训练与验证损失曲线，便于分析模型学习状态。
- **推理与测试**：集成了一套推理脚本，用于快速测试微调后模型的效果。（这个只在测试中，最终代码删除了）

**然而，在经历了多轮次的调试与优化后，我认为：Qwen-0.5B-Chat 这个特定的模型基座，对于小规模、高事实性的个人知识注入任务，表现出极强的“预训练惯性”，使用LoRA方式难有效改变其行为。**



## 项目历程与核心发现：

在微调过程中，我遇到了下面几个在LoRA实践中可能出现的问题：

1.  **复读机**：
    *   **初始问题**：模型只会重复单调的系统提示（System Prompt）。
    *   **解决方案**：通过实现正确的**标签屏蔽（Label Masking）**，确保模型只为“回答”部分计算损失，而不是为整个输入计算损失。这是所有指令微调的基础。

2.  **注入失败**：
    *   **问题**：训练损失几乎不变，模型权重没有更新。
    *   **解决方案**：通过打印模型结构，定位到Qwen模型的正确层级路径 (`model.layers...` 而非 `transformer.h...`)，并重写了LoRA注入脚本，确保了可训练参数被成功挂载到优化器上。

3.  **过拟合**：
    *   **问题**：训练损失持续下降，但验证损失在某个点后不降反升。
    *   **解决方案**：这揭示了**过拟合**的发生，并证明了我“根据验证集保存最佳模型”策略的正确性和重要性。

4.  **AI幻觉**：
    *   **问题**：模型开始产生有意义但错误的回答（如“hiahia是AI系统”、“hiahia是摇滚乐队”）。
    *   **解决方案**：我尝试了多种策略来对抗模型的“预训练惯性”，包括：
        *   **随机化System Prompt**：破坏模型的“背诵”捷径。
        *   **调整LoRA超参**：大幅增加 `lora_rank` 和 `lora_alpha`，提升LoRA模块的影响力。
        *   **优化训练策略**：引入学习率调度器、权重衰减等。
        *   **数据增强**：在数据集中加入“正反辩论”样本，直接对抗模型的错误认知。
        *   **格式简化**：放弃ChatML模板，回归最原始的文本续写模式。

## 最终结论与未来方向

尽管我采用了上述所有的微调策略，Qwen-0.5B-Chat模型依然未能准确学习到我注入的个人知识。这让我得出一个结论：

**工具与任务不匹配。** Qwen-0.5B-Chat作为一个经过深度对齐的**对话模型**，其内部权重对于“维持对话模式”的优先级，远高于“接纳新事实”。对于小规模的知识注入任务，LoRA的力量不足以改变其原本的行为模式。

**对于希望复现或继续此项工作的人，我提出以下建议：**

*   **更换模型基座**：将本项目的微调流水线，应用到一个更适合微调的基座模型（Base Model）上，而非Chat模型。

*   **作为学习资源**：本仓库的代码，尤其是`train.py`中`QAdataset`类的演变历史和`inject.py`的实现，是学习和理解LoRA微调细节的绝佳实践材料。

## 如何使用这个代码脚手架？

1.  **准备数据**：修改 `data.py`，放入你自己的问答数据。
2.  **配置参数**：在 `config.json` 中，修改 `model_name`为你选择的新模型基座，并调整 `lora_rank`、`lr`、`epoch` 等超参数。
3.  **下载模型**：确保将新模型的权重下载到 `model/` 文件夹下，或直接使用在线路径。
4.  **运行训练**：`python train.py`。


